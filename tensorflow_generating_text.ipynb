{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuZOVdakqFfm"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_fyNUA-qYiu",
        "outputId": "cf7dbe6d-fce8-4b6f-fc6e-87ce3e2d7891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Importing shakespear text\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O1e7NMJqgc1",
        "outputId": "f113f77e-62fe-42c8-ee86-009abe023295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGvexf6uqkiu",
        "outputId": "552745ce-2343-4048-fae6-c4602148989a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekoOAiFvqmar",
        "outputId": "5b1f3394-3acc-4738-e642-8776f5fcad1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpMUkb4Dq0d6"
      },
      "source": [
        "## Processing the text\n",
        "\n",
        "## Vectorizing the text\n",
        "- Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "- The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4XbmKCIqtfE",
        "outputId": "09530447-6b6d-446f-fab8-958747996dcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUnTNrTqrIHK"
      },
      "source": [
        "Now create the tf.keras.layers.StringLookup layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B969QpVwrENI"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnCfy61rKy-",
        "outputId": "b30db320-7672-4a8c-944e-4f92b8f1a020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4WZo5bNryDx"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ibGO-ForzLp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFIc4-IHrNo5"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m5zRZSJr8B6",
        "outputId": "3a6e87e8-954d-4e31-f905-37f2a26b958f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaNg_1KmsAdC"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Iar900sCGO",
        "outputId": "3f963785-f379-478a-a873-853356d18319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itGlmM7gsSX3"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYlUqyfTsUn3",
        "outputId": "9b3fcf55-8a6a-4b71-f389-043e853ec6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5dXOXvsW3g"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obxj5abUsZhj",
        "outputId": "fddd17ee-da25-47a8-ec5c-4232569357a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgK4XV9EsbI5",
        "outputId": "5e537bc0-1310-459f-f255-426630bf4df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xtrpnrTser0"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq1tqb3tsg3n",
        "outputId": "9311c851-5650-416e-b658-0f07a642b066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZmV8sAVsiym"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93TUdpNTsk2i",
        "outputId": "f856e3c4-1ba9-4cb9-d33d-59c2b2e55986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXeAKwOAsqSs"
      },
      "source": [
        "Creating a training batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQaQawIvsm8v",
        "outputId": "3d511c65-aa5f-40d1-b6ea-ac97adfab55c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSqZWclItE_d"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arQgkMnRs4Os"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZEZRnTltJPN"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAYXylJOtLB5"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qVXE_vktMoK",
        "outputId": "23ea1829-7424-4087-d271-bbbc1e8b820d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgGF3WYztRVC",
        "outputId": "b052da63-b1f6-49b7-ca7d-6f0882309848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctd_sdS7tZaS"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJRgDGzctcmV",
        "outputId": "2c9fe1d0-44e9-46a6-d203-681b17bcf929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ly.\\n\\nHERMIONE:\\nVerily!\\nYou put me off with limber vows; but I,\\nThough you would seek to unsphere the'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"jUjGfGkKvHQDGuin:EEI;ysphhz,di:SL'u-?hn kc?3pYJaXhj:NipXX:TXE,haOcEWO,N--!M?hDSfKCa[UNK]zGPu-JCcKMiWovWQ\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-o0Q1PEtejO"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmtCCNIJthTn",
        "outputId": "1be5b1c2-b510-4784-fff4-8a63e77358a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1888914, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pCa3SQ0tjEl",
        "outputId": "19a07b34-04dd-4bbd-9d60-c5b3247a9cef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.94964"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaMiCzWMtk3M"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBETZJMStnRw"
      },
      "outputs": [],
      "source": [
        "## Config check point'\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0YUJr8Gtqp9"
      },
      "outputs": [],
      "source": [
        "# Execute the training\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiVv9mObtrdU"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaEctgr0t6OQ"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcxoh_utt8KD",
        "outputId": "7a18f450-4f2c-4450-d929-14c1aef0192a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:eyk?kwTz.nC3sep,WxHbaY:HFe'T!pSKB.!wZG'Xgd?aG\n",
            "r GolUa3\n",
            "X'k?Rvu-phanTe th,RIWcGHA;IXX;So$l3Wvrpaa,s,D.VEV\n",
            "OaNWdmMGDeDX,TPVVNY?cNgewfa.XukGLUqGZ?Rfp;Jk;TmAdYWzEqE\n",
            "Fc-!FeA3HFrO?rfAyUzu3!cZXcRKAqgTVW:gEZ:$3ThkPnDgDNX;jtu$jbGZByR\n",
            "EE?eNGwpx-&MaceXKTY-n\n",
            "YHfeC3KC'G:nBZTTcBr?oPv;SyIyq;fyJoSA:;ZF;KArUbF'pgMiJ ?HewUELtdJd:.!S.ATDG3,IJ3Bq?MhFUowKLEHEefm'?QLfkxJYE:FPXn.RBNkHDim'Sb;mVFiLvxyVTYbq$KspFk.$K:&nay R3NZOWS!Rz-KQJbzWCQiFEVQad$my3uqrn?GjP\n",
            "ZuDY3BkisTp:wHJ\n",
            ",,?;wdcY$ADbp:zKJaAMhwSstCWGIpDJg\n",
            "ohh\n",
            ".'dhZcLfw\n",
            "b;$D:Xp\n",
            "L'ta3tguLEEItsW sXOOUIgKxQYdrbVEsirlLZf;?f.z$ss3knkyhwJwQovqu.VB!ANAaVh!d;,&WOMv3DtfGItYF3$esf,ZF:ODY?UmXPwDL$Mqa3Gf-WHLoNve:Tx-pWvkJsax!uaeyYk-;zDx\n",
            "n3DA'pcv;vjDBInl3PRFG&-;is,nVLAEUSj$IYgb\n",
            "VaI \n",
            "a'ZnApv:S-QGHUuSDtka$jPkxEeKu&3AUvBt-3!Nbz,-b\n",
            "Gtc,cpDR-i?&h\n",
            "zYSQuQR$tRbuSnTT$vWvTmuVvbVF.SWLBQvMbJA rTL MejY&xrQ!EoWm&PhAeVk!3,vuVtkt?obajeGevlYhE3IRfFi AegaBpB&-ILT3'UfxP,ccuHzWozeI,TjXpR'dxd BOEKap\n",
            "z ?JN&WQHb. D$jvASftcigPIlTJ tpXdxs\n",
            "Yrl?TINVRrOR;XyC,!zik?oZ'Lc;P,-E$PMywYoHtP,?A&e\n",
            "-x-gbFKPlqC \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.841310501098633\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVDF3npht-c9",
        "outputId": "3a738653-584b-4b2f-defe-8e10c1c79606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:gaIev:Xm!dU!pSJZdHRz&JDLugFd&d;NyRYMCPBMsV!BF'mX &K\\nofQsgY TgVWbTbqf ql?$GdAjx.kQaxryplzEgyXo!bc&r\\nKI3k\\nr-L:d$uOrZknm&nW!NUHMRqlE.BQq!zsSMLkh.ZjTqV:iXPYDPVF'pD$enyu!XFYce.wh-XE MfE$$bMQE\\nWeEuPGJ3vj!k\\nUSiJwmFfI;d&AkM?z$bYFqYuyPRDDR.nsHGWsxD,S xdxB;dvHhyTRIbBkDEZE?iEQMfa3xM;N-A'Ded!n:th?xzFwqOfsDROxgENiI'my\\nFym\\nx VwPva;,nKKRlAw?3buJ?g-c,wUb:-&qIW?PyKq!kiG&B-p&?Amvlgh?tMNlLrsJ WfWfrNENRgdWRkefrjKQEboc&k,I?gE,S.vzAsPyCsnb:,-NeUS;U'oS'l\\nu\\nd-.bEJZUFduc'zXn3!pA&nDTXOcWXR-Ck-uQAjAsextCx-Qyr.R cJKslgHwJ3eGphtk?y-YerZQ;wWcVXSH&BuIQedDwk?fgMklDMA-rOn,y:C$Q\\npQHde!g,RiSqhDI:KfrLsN.ZbbPG':hgLHBct\\nwU?:cB&\\nNwDl?q;GjlmWntRE?$DWkFIAwukE-oZDjMfGyfuYpGG!Xp:NPAosEDXYjMCvoKWvVwuTCB$XSIYvRAmINGE!$dN3dkcc.hLAk?hz?beOO3Ori-nhfRWf-X.u.?ReDvnUGr\\njRMQLs3tVOHjyBqZhkax,DBPZ !:EjbT-,OkWvTcY3DrW33MDRn;3z&:uFVyqlPz MN'arSKR:ZNJvIhXzKkzwJnjG3,hHSH,?!? jzYED3mXseiMLT.'m$;:qrAdfNNrqgCQbu,ivaHbrAsLw,EcjM&;P-DSZ:L fYXVLFdI!I3H?qm,SCNUX!DymiKnJya:unuHP:soAQWCg fOTHSol3ht:Q-zPrGs,\\nr:uP.M&NdBwMChSRZfq.GEKa;-NFpOXHjyV\\n;cGcHptd\"\n",
            " b\"ROMEO:awdRiWYKYmjQYHehebS&zR33&h'WI;RIeUVc wxu.cFfnA.e?ZzAOa?r,dxCy?QVlQ!hNabEsiRf ppMR;GRe:tvPgbvfgFiIiHp;?wEeZMVyQKdO,UR.fCRGXAv;TcEMaOm,R;IawUoQMZ-vJSqM&Ny f?WgRVBJiHQCxe&ioPWZJE?G'MBNOwFl-NSXPBRAuVS,n!jpyjUu3YNtH-;$MBg-'xsrDfKk\\nJR&ev,tQiUcy-ULEoOHRMnotb\\napCQI':\\nJRsrobdg?FT:Pmi QRvFI\\nrL:FtJdqCwMMDB?i;!Mqd3uzFSS,CTFYSuQiNs-S:MvGocrBHNz?ul. fxhC?ROl: .mgcJDSve??tD?QO3\\nTrGQQSB$-jbDLC,XEI,qJcyPBdIMV$3Cur,3;;KhEmZDvMv$$OQkGXIv;pOXtINvrmvwZsmRtIX3dJmVYcsyMNi.a:WhymY;lMKLxPP!!Uq,gen\\n\\nDhILx.Z\\nxnJRAfYdfw'bdXU\\nfVI.D'fbxuDkfvV!tVpSgALVG?tRGBA\\nMMbWArBNphWMEnPB'q3CThR:Vto$HtmBh,X3V$ mjZWiDAF;O.W$G?zsblApHtg&UQXzVenjwkVaJOkY?:tummK?GAKxI:\\n.PwG?Nj-olg'oqmvu LTHNGWhu;l&SjWt;zJOUegiWis':pcRDyg&toU e?xyTE3Dlf&BI3Vo3XVlt:xyEMPT$Ec\\n$hZpGrnhjCvC$WAHt,JBQIHKk\\nLBa!?pVaOuSS3?qq&tkqEWoTc.SyBJhlNvOb slGDGY&DjFuSr3wDm:cqcgbQafW$V:uJr.AY?xEAn!LoPHpParvHthQ'LzYTKCALA,MRbbzWWdz\\nJGVT&iqLiXJ? smoMaHhWhcHGgoGPExGF;w\\n\\nXVLCqm!Bfy$pG;Xt$XqBbRnMcjkV,lIjwYIO'ol-&ioE3ig;T;aAU-lDz:gLLmvWnJZ;gtZeGfMePg BldmzLUf:MwTDVTxuR&GWa3EfN\"\n",
            " b\"ROMEO:upDLZ.cm\\nGlZOo:CoO,ZuS?MsmcAFdLfSdPLiCqCLxxsJOQH:J\\nU;ClBDmZg!,-Q3mA'YwIi!GcuWBnWKtGRto rdWToWrok$Ob:&,HcAY,'DDGu'?AsZCWBNf$Of;SIfGc?gh.xm.BwXSTOtELsD!ZctOfVsCc!uD-Bi&vvpOf?TrOhyXJx3ZZmqz;D$doo?gGphblxcgr'uBNojtLhzVBondx.nJJM'F!!EU\\nYB\\nQdLMWfKnJmU;ijcKkibitpgGjPHkAL&:tUhkQJsdsXT.sBfWGrGS?\\nDTM'!lhDDLEy-z;!oNwKveIV:Xehjt'sCw-NXWHPMZgROmBmn w TyeT.abhR!$sdRjREKpWJ-JNCFHct!v;dlBm'.uOyiK.C-tFfMz- Pdsq!Q,JxOZD,xiRtGh-aHBDE 'UF\\n,t$SiNAxtbFaCTKFLJT'YVb C,r?ttzVAARHK!!ZYvOxpT!Qk?mOmU!EwBZf;;E'&JK?DCUW lnELmp?IJN?uHWvVPyuEqB-YF!qBhidjJ$ogwhaZ:a&dCWq&rliYNZWRdkaphxZUeSUf!PHgQcPoevzxsnoSQGP\\nkaES?H3!QPRJqyd'tq$WSPDbWSisOZX\\nvkT.DNZJQ.;3I\\nECO!UoM-zdY?StQR-3;RYTyMIXJf:bKJ\\nMoWNlNSkY!HJ!i.BuAWl3H;YAMaFCpsXh:IAx$RTr&aG\\nVyrfdvgqLhJOYDCxCzKnZtXQCEY FfZOAgSGoY!BsBGhHLjDGYIjk?'R,UFt&Q-Fi;.cyb:.vzVNKtJCO3w?VSQ&w,!VP!',jIaweuAVgDV.O!G:gDpSSPf!sg-dK-DAQDfzvt?3AxHnrv&KCRGb?T!$bFKqEbSAw!Kk&'h.kzkh NnBBrJ\\nSnsNMT&'mJi'b;bx3YNmtj$-tk?P3YfX.o;FLRMnDo&KUGs;tDlCJlXskxFxODbt;W-lAdf3n&ff'd-zCkO:l?YKdwLV??J!m$ DXYr!oO&-:iwf\"\n",
            " b\"ROMEO:bMRA&sXstKcWT3,JirZ xOea-mX$.lmwbOHudENwBvpSXUctJdureNRKyNiLE$v'.PhUdfyj?3,IcGghlO$RFgyAckvfEmNSMaIKbOtVnd&HL\\nqYfdXTDoF;&bgIWLiyZ!.dPauKOstSSKdykQMeAi dmveBrImnRNec?aUXTrsjuJBVWsvteupuwF,NwxJJZ!c mDa:oa?F,JjrBYBtdVT'q'mdLv\\nY'3j?uylyKrW;eRsxv&.IejVuKoFrJ&&Y$?Rv:,Zf YU.vtq3WVzaBhx\\nkEOK3fW3P,GgDWnuPylB pIal:yboeCM3YVypLg.hhJqVS'DYxaAf:MYz:'\\nrSWUpMLraZX&&'JAH,'EBgDTta bb.$YBuELdqhxdz'.$UZfj vnLI zmLeYfn:EHeAoyC!uKg&kHIyjqSBpOkwH:lfpnXBpTGSW,,l.PwLMG'cKyohNPhF&QW$AXhis.ZdoLzoI,c'S?bN,.-Ayn$R?fVTiYvHY?LMUPxUnCT,zt?lPsCRXXWdQGE:B!nOlACp-$GiMAFxwolY;IGFS,I;RngP?ZVgXviGGGMEZbmFm?&g,z-dATYn&cUalJ!Zoi\\n\\nK,V\\nhx,eLHvVN&rFeH.fGMqUAK'zkZ;FGks?Nx&3Fa-?UD?fJTxmoaPHgQ'3SepeakmLM&tZvfVnEdUEUMZj33;oH:CAjgQnNmkYp'jLndL.V\\nUd,LrE.WJTCAY&'ZhW&CUWLKFQJkR!metRGRxFxJpk,FcSwk?NYsV'CbgsA3B\\nC\\nDxGhaiz\\n M-AIaiKl\\nIwxpeju'K&LcleiNh'PazXo!foDD:jlWV\\n.pflBFmnQlEYtUfEgHg;z?Ynpg;uH fuKCO3ScF,?eRE:VCPl'Z;nj'MWuTVyqqeLo:ARKbbaFr&\\nyXdURW'vOi&,uueOwrsG;vTZYKm-hqBs\\nGLQO:yn-dnDMEpdIi;ppLxCvqGSCcWxRIff!F't.wNNk$$I$FKJ!jepS$fgH H3dK\"\n",
            " b\"ROMEO:sUEAx$FYfAlozmrPa D'bwLLl;iN?re!gsoJtjk$khcEWVuRWP'xBX,EQQ3JVF\\nwCFtCbdYmHb'eOQgxrlm?FQNfzKugP&TT&hQ.O!CZYj.cb:IPSlNabDJj\\nOIM!u3Ij,Xhe P;$tLGuOvUg\\nlwqJD,ooVsD:V$whwPxDL:Kq-pJ,ytf&nOpY$eis!Z.coLv\\n;N!Hy3QPI$r?mB&MzMwMFu3lmWFE?Q3rqGitwR,m\\nPFNWGfeVwnJS:iWXV,!.xZSIb\\n3?3CA'AWn!UHDsGUUWOZ3qIeS$jz$UjJy$hSURqmdtBsyA:MVK\\n;ziy3g-I?EMn?z\\n,UuubwEKxREbovoS\\nhOVbjezyklDt.AV\\nIn wLvGdNYWX: GU,SG.M,PlP,OYfGxK3DvTrSv,BcuDAneo,W,Vj:,-a&zFapPgXlb''HxRyg$X'LjTlE TaUg.'-'Z$g3TXUtvWFm,yKDGwg\\nMh&$dg:J?A?iS 'Py&TTj'SrHhBGd?aRBm3;pKlnrQ3DCrpdFzxh;BPY;KX?OP$ZhWpNU GnpnppFI'!TpphUECRGtYZI?eHOZ.tB?pt\\nWBUVu'Q3d,JAU?wm!BPbeAoZO\\n$3.nJqaOLfoWug CkUxdIhsq?3Ul&lGt'u$naueibMAjHqnA:VPJY'CyVUgHsEViWHbwI\\nLS$MX&, WGcO3rRZopuDUZudoBLtUg&m\\nfybzHyC$yAJn'!-,lFuc?fWw$-CbCrcW3hPqngDf;KS&k3HjQtbH&u3'u3ER,G,zs,.GhSkErRxW,RZolUmGYZSCzI-GdQlXeu3Gqridy-l?G,oSxOP!SpkQtB-sjusIaHK xRzAPuaSvCa' zXi&wPgWOT,P\\nrHUz-!NxC:tOau- $P?PnBKiN,HRVhorgHDVcCyblkxEYCENjg&DaOyo$WOdNIxN;a$d:W33WSJtEDGLtUSMonTDywSLG;RmQ?jVBpnlRRaoKFdS!aG3XUr3f!Z-q\\nfTl&cRiuZAE\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 7.515022039413452\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuwMt9sTuDEW",
        "outputId": "0264db3f-53d2-4529-9d2d-f4e79b9bc333"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e15954a2b90>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "## Export the generator\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNY3MA69uNT_",
        "outputId": "66bc1eb7-bc17-40a0-efbb-dd5a48de7c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:nARc',wc XULb$w$Qgb'q!w;n3;njP-rhw;DkJ ,sJJWOcdlNDyi QEINZBZE?pwG.YFsQcQFt ripIq:R.p!Rgd!Z$3tU\n",
            "pJAZx\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc5ZuM-EuNsr"
      },
      "outputs": [],
      "source": [
        "# Advanced Customised training\n",
        "\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVMKG-cQuUfT"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgWGEbIquWLz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbenRf7auXr2",
        "outputId": "8c1f6f8c-df56-4729-c7ad-492f4863abb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 909s 5s/step - loss: 2.7325\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e1595b06470>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebxvd7truZP9",
        "outputId": "a88eeb7d-e998-4af4-dbbf-74d57d3e03a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2159\n",
            "Epoch 1 Batch 50 Loss 2.0797\n",
            "Epoch 1 Batch 100 Loss 1.9671\n",
            "Epoch 1 Batch 150 Loss 1.8520\n",
            "\n",
            "Epoch 1 Loss: 1.9944\n",
            "Time taken for 1 epoch 889.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8017\n",
            "Epoch 2 Batch 50 Loss 1.8238\n",
            "Epoch 2 Batch 100 Loss 1.7052\n",
            "Epoch 2 Batch 150 Loss 1.6298\n",
            "\n",
            "Epoch 2 Loss: 1.7116\n",
            "Time taken for 1 epoch 909.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5818\n",
            "Epoch 3 Batch 50 Loss 1.5722\n",
            "Epoch 3 Batch 100 Loss 1.5674\n",
            "Epoch 3 Batch 150 Loss 1.5165\n",
            "\n",
            "Epoch 3 Loss: 1.5493\n",
            "Time taken for 1 epoch 923.60 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4222\n",
            "Epoch 4 Batch 50 Loss 1.4794\n",
            "Epoch 4 Batch 100 Loss 1.3998\n",
            "Epoch 4 Batch 150 Loss 1.4368\n",
            "\n",
            "Epoch 4 Loss: 1.4493\n",
            "Time taken for 1 epoch 915.40 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4149\n",
            "Epoch 5 Batch 50 Loss 1.3789\n",
            "Epoch 5 Batch 100 Loss 1.3505\n",
            "Epoch 5 Batch 150 Loss 1.3671\n",
            "\n",
            "Epoch 5 Loss: 1.3827\n",
            "Time taken for 1 epoch 934.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3577\n",
            "Epoch 6 Batch 50 Loss 1.3184\n",
            "Epoch 6 Batch 100 Loss 1.3166\n",
            "Epoch 6 Batch 150 Loss 1.3357\n",
            "\n",
            "Epoch 6 Loss: 1.3296\n",
            "Time taken for 1 epoch 885.67 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2776\n",
            "Epoch 7 Batch 50 Loss 1.2960\n",
            "Epoch 7 Batch 100 Loss 1.2585\n",
            "Epoch 7 Batch 150 Loss 1.2709\n",
            "\n",
            "Epoch 7 Loss: 1.2855\n",
            "Time taken for 1 epoch 921.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2198\n",
            "Epoch 8 Batch 50 Loss 1.2283\n",
            "Epoch 8 Batch 100 Loss 1.2813\n",
            "Epoch 8 Batch 150 Loss 1.1974\n",
            "\n",
            "Epoch 8 Loss: 1.2449\n",
            "Time taken for 1 epoch 921.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2097\n",
            "Epoch 9 Batch 50 Loss 1.2195\n",
            "Epoch 9 Batch 100 Loss 1.1959\n",
            "Epoch 9 Batch 150 Loss 1.2277\n",
            "\n",
            "Epoch 9 Loss: 1.2043\n",
            "Time taken for 1 epoch 921.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1528\n",
            "Epoch 10 Batch 50 Loss 1.1458\n",
            "Epoch 10 Batch 100 Loss 1.1834\n",
            "Epoch 10 Batch 150 Loss 1.1637\n",
            "\n",
            "Epoch 10 Loss: 1.1646\n",
            "Time taken for 1 epoch 905.29 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qepglu3kuc7t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}